**《人工智能基础》CIFAR-10项目文档**

**1、背景分析**

`	`CIFAR-10是一个更接近普适物体的**彩色**图像数据集。CIFAR-10 是由Hinton 的学生Alex Krizhevsky 和Ilya Sutskever 整理的一个用于识别普适物体的小型数据集。一共包含10 个类别的RGB 彩色图片：飞机（ airplane ）、汽车（ automobile ）、鸟类（ bird ）、猫（ cat ）、鹿（ deer ）、狗（ dog ）、蛙类（ frog ）、马（ horse ）、船（ ship ）和卡车（ truck ）。每个图片的尺寸为32 × 32 ，每个类别有6000个图像，数据集中一共有**50000** 张训练图片和**10000** 张测试图片。

**2、目标分析**
**
`	`CIFAR-10比赛的目标是通过训练机器学习模型，使用CIFAR-10数据集中的图像作为训练集，能够准确地对CIFAR-10数据集中的**10个类别**的图像进行分类。我们需要构建一个高准确率的模型，利用各种机器学习和深度学习技术，并优化模型以提高分类准确率和泛化能力。

**3、数据收集和预处理**
**
`	`数据集来自kaggle/c/cifar10，同时在一些部分使用了cifar100数据集进行预训练。

`	`数据集结构如图：

数据集结构

**	其中test.7z, train.7z, trainLabels.csv, sanpleSubmission.csv 为kaggle中下载的数据，解压数据到test, train 文件夹。

`	`train\_valid\_test文件夹结构如图：

文件夹结构

**	其中train\_valid为20epoch测试训练的训练集，valid为验证集。train为总训练集，用以完整训练模型。test为测试集，用以最后预测最终输出submission.csv文件。

**4、训练集、测试集描述**
**
`	`首先进行图像增广提高模型泛化能力，训练集在高度和宽度上将图像放大到40像素的正方形，随机裁剪出一个高度和宽度均为40像素的正方形图像，生成一个面积为原始图像面积0.64～1倍的小正方形，然后将其缩放为高度和宽度均为32像素的正方形（尝试过resize到(224,224)，但效果一般）。执行随机水平翻转图像，对彩色图像的三个RGB通道执行标准化。

`	`验证集测试集仅执行标准化。

**5、模型选择及优化**
**
`	`模型上选择了在d2l的ResNet18的基础上进行改进，增加了通道注意力机制和空间注意力机制，还有较之更深的ResNet56。

`	`经过修改的ResNet18模型引入了**通道注意力机制**和**空间注意力机制**，以增强特征表示能力。通道注意力机制通过自适应平均池化和最大池化结合，利用两个卷积层和ReLU激活函数，生成通道注意力图。


通道注意力


空间注意力

空间注意力机制则通过平均池化和最大池化后的拼接，再经过一个卷积层，生成空间注意力图。这两种注意力机制可以有效地增强重要特征的权重。模型总体结构包括**基础卷积层**、**四个残差块**（每个块内包含多个残差单元），并在残差单元中应用**通道和空间注意力机制**，最后通过全局平均**池化**和**全连接**层输出分类结果。这种结构设计旨在提升模型对重要特征的敏感度，增强识别准确率。

`	`ResNet56模基于经典设计，采用了BasicBlock模块和更深的层数来增强模型的表征能力。模型由一个初始卷积层和批归一化层开始，接着是三个由BasicBlock组成的**残差层**，每层包含9个**BasicBlock**。每个BasicBlock包含两个3x3卷积层，后接**批归一化**和**ReLU激活**函数。下采样通过在每个残差层的第一个块中使用**卷积层**来实现。最终通过全局平均**池化**和**全连接**层实现分类。

ResNet56比ResNet18有更多的层数（56层对比18层），这使得它可以学习更复杂的特征表示，提升模型的表现力和分类性能。由于没有引入通道注意力和空间注意力机制，可以说ResNet56在更深层数上提供了**更高的特征学习能力**，而改进的ResNet18通过注意力机制增强了**特征提取**的有效性。

**6、超参数设置**
**
`	`ResNet56相对于ResNet18具有更深的网络结构，拥有更多的层和参数。较大的学习率（**2e-4**）可以在训练过程中提供更大的步长，帮助模型更快地进行**参数更新**，以适应较大的网络结构和更复杂的特征表示。对于ResNet18，较小的学习率（**1e-4**）可能更适合，因为它相对较浅，参数较少。较小的学习率可以帮助模型更**稳定地收敛**，并减少过拟合的风险。

`	`因此项目训练中ResNet56采用2e-4的学习率，ResNet18采用1e-4的学习率。由于cifar10数据集不算小，所以设置**batch-size**为128，**lr\_period**为4，**lr\_decay**为 0.9，**weight decay**为5e-4。测试训练均为20epoch，ResNet56完整训练为**50epoch**，ResNet18完整训练为**40epoch**。

**7、方法分析**
**
`	`仍旧采用d2l里的训练函数`train()`:


训练函数

**	加入新的bool参数`savemodel`，若置为True则会在训练时每五个epoch记录一次模型，便于回溯模型性能。

测试了多个优化器包括：**Adagrad**、**Adadelta**、**RMSprop**、**Adam**，由于SGD更新速度慢且难以收敛，因此优化器由SGD改为**RMSprop**。

**8、过程分析**
**
`	`首先训练改进的resnet18，无需保存模型：


Resnet18训练代码

（后续类似不再给出）

`	`观察到模型收敛速度较快，20epoch已经达到还可以的效果，此时loss已经降到0.179，预估在25-30epoch可以达到最高准确率而不会过拟合。

Resnet18测试训练20epoch

观察到模型因为深度较大，即便是学习率较高仍有收敛速度较慢的情况，20epoch时 loss仅仅降到0.6，而训练准确度已达0.8。预估在35-45epoch可以达到最高准确率而不会过拟合，接下来对resnet56模型进行50epoch的完整训练集训练。

Resnet56测试训练20epoch

完整训练模型50epoch，每5个epoch保存一次完整的模型权重数据。观察到50epoch模型loss降至0.2并且趋于平缓，将其看作已经收敛，随后使用已经保存的模型权重30、35、40、45、50分别预测输出submisson，到kaggle测试准确率。

resnet56完整训练50epoch

**9、结果分析**

resnet18的kaggle得分：


改进后的resnet18由于添加了通道注意力机制和空间注意力机制，效果比较原始的resnet18 0.87的分数有所提高。

resnet56的kaggle得分：


**	resnet56的深度较resnet18更深，学习到的特征更多，需要的训练时间更长，效果上会略好一些。在图中可以见到50epoch的正确率仍然比40epoch要高，可见模型还未过拟合，可以多执行几个epoch效果可能会更好一点。
